# -*- coding: utf-8 -*-
"""LightGBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eiEimzhEx7XJYhLCXek801u2LhQnGco1
"""

!pip install -U dask

!pip install catboost
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
warnings.simplefilter(action='ignore', category=Warning)

df = pd.read_csv("/content/drive/MyDrive/Datasets/diabetes.csv")

y = df["Outcome"]
X = df.drop(["Outcome"], axis=1)

lgbm_model = LGBMClassifier(random_state=17)
lgbm_model.get_params()

cv_result = cross_validate(lgbm_model, X, y, cv=10, scoring=["accuracy", "f1", "roc_auc"])
print(cv_result['test_accuracy'].mean())
print(cv_result['test_f1'].mean())
print(cv_result['test_roc_auc'].mean())

lgbm_params = {"learning_rate" : [0.1, 0.01, 0.001],
               'n_estimators': [100, 200, 500, 1000],
               'colsample_bytree': [0.5, 0.7, 1.0]}

lgbm_best_grid = GridSearchCV(lgbm_model, lgbm_params, cv=10, n_jobs=-1, verbose=True).fit(X, y)

lgbm_best_grid.best_params_

lgbm_final = lgbm_model.set_params(**lgbm_best_grid.best_params_, random_state=17).fit(X, y)



cv_result = cross_validate(lgbm_final, X, y, cv=10, scoring=["accuracy", "f1", "roc_auc"])
print(cv_result['test_accuracy'].mean())
print(cv_result['test_f1'].mean())
print(cv_result['test_roc_auc'].mean())

# n_estimators is important for LightGBM algorithm